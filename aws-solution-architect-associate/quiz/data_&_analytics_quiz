1. You would like to have a database that is efficient at performing analytical queries on large sets of columnar data. You would like to connect to this Data Warehouse using a reporting and dashboard tool such as Amazon QuickSight. Which AWS technology do you recommend?
a. RDS
b. S3
c. Redshift
d. Neptune
Ans:
c 

2. You have a lot of log files stored in an S3 bucket that you want to perform a quick analysis, if possible Serverless, to filter the logs and find users that attempted to make an unauthorized action. Which AWS service allows you to do so?
a. DynamoDB
b. Redshift
c. Glacier
d. Athena
Ans:
d 

3. As a Solutions Architect, you have been instructed to prepare a disaster recovery plan for a Redshift cluster. What should you do?
a. Enable Muli-AZ
b. Enable Automated snapshots, then configure your ResShift cluster to automatically copy snapshots to another AWS region
c. Take a snapshot then restore to a Redshift Global cluster
Ans:
b 

4. Which feature in Redshift forces all COPY and UNLOAD traffic moving between your cluster and data repositories through your VPCs?
a. Enhanced VPC routing
b. Improved VPC routing
c. Redshift Spectrum

5. You are running a gaming website that is using DynamoDB as its data store. Users have been asking for a search feature to find other gamers by name, with partial matches if possible. Which AWS technology do you recommend to implement this feature?
a. DynamoDB
b. Redshift
c. OpenSearch
d. Neptune
Ans:
c 

6. An AWS service allows you to create, run, and monitor ETL (extract, transform, and load) jobs in a few clicks.
a. Glue
b. Redshift
c. RDS
d. DynamoDB
Ans:
a 

7. A company is using AWS to host its public websites and internal applications. Those different websites and applications generate a lot of logs and traces. There is a requirement to centrally store those logs and efficiently search and analyze those logs in real-time for detection of any errors and if there is a threat. Which AWS service can help them efficiently store and analyze logs?
a. S3
b. OpenSearch Service
c. ElasticCache
d. QLDB
And:
b 

8. ____ makes it easy and cost-effective for data engineers and analysts to run applications built using open source big data frameworks such as Apache Spark, Hive, or Presto without having to operate or manage clusters.
a. Lambda
b. EMR
c. Athena
d. Opensearch Service
Ans:
b 

9. An e-commerce company has all its historical data such as orders, customers, revenues, and sales for the previous years hosted on a Redshift cluster. There is a requirement to generate some dashboards and reports indicating the revenues from the previous years and the total sales, so it will be easy to define the requirements for the next year. The DevOps team is assigned to find an AWS service that can help define those dashboards and have native integration with Redshift. Which AWS service is best suited?
a. OpenSearch service
b. Athena
c. QuickSight
d. EMR
Ans:
c 

10. Which AWS Glue feature allows you to save and track the data that has already been processed during a previous run of a Glue ETL job?
a. Glue Job Bookmarks
b. Glue Elastic Views
c. Streaming ETL
d. DataBrew
Ans:
a 

11. You are a DevOps engineer in a machine learning company which 3 TB of JSON files stored in an S3 bucket. There’s a requirement to do some analytics on those files using Amazon Athena and you have been tasked to find a way to convert those files’ format from JSON to Apache Parquet. Which AWS service is best suited?
a. S3 Object versioning
b. Kinesis Data streams
c. MSK
d. Glue
Ans:
d 

12. You have an on-premises application that is used together with an on-premises Apache Kafka to receive a stream of clickstream events from multiple websites. You have been tasked to migrate this application as soon as possible without any code changes. You decided to host the application on an EC2 instance. What is the best option you recommend to migrate Apache Kafka?
a. Data streams
b. Glue
c. MSK
d. Data Analytics
Ans:
c 

13. You have data stored in RDS, S3 buckets and you are using AWS Lake Formation as a data lake to collect, move and catalog data so you can do some analytics. You have a lot of big data and ML engineers in the company and you want to control access to part of the data as it might contain sensitive information. What can you use?
a. Lake formation fine grained access control
b. Cognito
c. Shield
d. S3 Object Lock
Ans:









