# 7. Amaozon S3 (Simple storage Service)

## 7.1 S3 Overview

**Basics**  
 - S3 is one of the main building blocks of AWS
 - Its advertised as "infinity scaling" storage
 - Many website use Amazon S3 as a backbone
 - Many AWS Services uses Amazon S3 as an integration as well
 - EBS snapshots are stored in S3 in backend
 - Certified cloud Practitioner requires deeper knowledge of S3

**Use Cases**  
 - Backup and Storage
 - Disaster recovery
 - Archive
 - Hybrid Cloud Storage
 - Application Hosting
 - Media hosting
 - Data lakes and big data analytics
 - Software delivery
 - Static website

**Buckets**  
 - Amazon S3 allows people to store object (files) in "buckets" (directories)
 - Buckets must have a globally unique name (across all regions and all accounts)
 - Bucktes are defined at the region level
 - S3 looks like a global service but buckets are created in a region
 - Naming Convention
   - No uppercase
   - No Underscore
   - 3-62 characters long
   - Not an IP
   - Must start with lowercase letter or number

**Objects**  
 - Objects (files) have a key
 - The key is the full path:
   - s3://my-bucket/my_file.txt
   - s3://my-bucket/my_folder/another_folder/my_file.txt
 - The key is composed of `prefix` + *object name*
   - s3://my-bucket/`my_folder/another_folder/`*my_file.txt*
 - Theres no concept of "directories" within buckets (although the UI will trick you to think otherwise)
 - Just keys with very long names that contains slashes ("/")
 - Object values are the content of the body
   - Max Object size is 5TB (5000GB)
   - If uploading more that 5 GB, must use "multi-part upload"
 - Metadata (list of text key/value pairs - system or metadata)
 - Tags (Unicode key / value pair - upto 10) - useful for security/lifecycle
 - Version ID (if Versioning is enabled)

**S3 Hands on**  

 - S3 -> buckets -> Create Bucket -> provide uniques bucket name and region along with default options
 - [Bucket naming rules](https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules)
 - Add files -> select file -> upload
 - Now, if you select the uploaded file, you will get the details about that file
 - You will see the Object url under Object Overview heading in Properties tag
 - Now, as we had chosen all the default options that means block public access included in it and hence if you click on object URL, you will not be able to access the same
 - Whereas clicking on Object Actions -> Open will open the file in new tab

## 7.1 Bucket Policy

**S3 Security**  
 - User Based
   - IAM policies - Which API calls should be allowed for a specific user from IAM console
 Resource Based
   - Bucket Policies - bucket wide rules from the S3 console - allows cross account
   - Object Access Control List (ACL) - Fine grained
   - Bucket Access Control List (ACL) - Less Common

 - Note: An IAM principal can access an S3 object if
   - the user IAM permissions allow it **OR** the resource policy allows it
   - **AND** there's no explicit DENY
 - Encryption: encrypt objects in Amazon S3 using encryption keys

**Examples**  
 - Public Access ->   Use Bucket policy (Allow S3 bucket policy)
 - IAM User Access -> IAM Policy only will be sufficient
 - EC2 Instance -> Create EC2 instance role and assign IAM permissions to this role
 - Cross-Account access -> S3 bucket policy allow cross account policy

**Policy**  
 - JSON based Policies
   - Resources: buckets and Objects
   - Actions: Set of API to allow or Deny
   - Effect: Allow/Deny
   - Principal: The Account or user to apply the policy to
 - Use S3 bucket for policy to:
   - Grant public access to the bucket
   - Force Objects to be encrypted at upload
   - Grant Access to another Account (Cross Account)

**Bucket Settings for Block public access**  
[Screenshot]

 - These settings were created to prevent company data leaks
 - If you know your bucket should never be public, leave thse on
 - Can be set at the account level

**Hands On**  

 - go to S3 bucket created above -> permissions -> Bucket policy
 - To enable this, you have to uncheck "block public access" policies
 - Once done, you can click on edit button in edit bucket policy 
   - Select "policy generator" as it provides a nice ui to generate a policy -> select
   - Also notice you have Bucket ARN that will be used in next steps
   - Select type of policy: S3 bucket policy
   - Principal: *
   - Actions: getObject
   - ARN: copy from the step above with `/*`
   - Add statement
   - Generate Policy -> copy content 
   - now go back to previous tab and update the copied policy and save
   - now you will have the objects inside this bucket as accessible

## 7.2 S3 Websites

**basics**  
 - S3 can host static websites and have them accessible on the www
 - The website URL will be:
   - <bucket-name>.s3-website-<aws-region>.amazonaws.com or
   - <bucket-name>.s3-website.<aws-region>.amazonaws.com
 - If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads

**hands on**  
 - upload your static content
 - in Properties tab -> Static website hosting -> Enable
 - host a static wbesite
 - provide name of your index and error document
 - save
 - now you get the url for the newly hosted website
 
## 7.3 S3 Versioning

**basics**
 - you can version your files in Amazon S3
 - It is enabled at the biucket level
 - Same key overrite will increment the version: 1,2,3..
 - It is best practice to version your buckets
   - protect against unintended deletes (ability to restore a version)
   - Easy roll back to previous version
 - Notes: 
   - Any file that is not versioned prior to enabling versioning will have version null
   - Suspending versioning does not delete the previous versions

**hands on**
 - S3 -> Select the bucket you created above -> properties tab -> Bucket versioning -> edit -> Enable -> Save changes
 - Now any file uploaded from now on will have version in it
 - you can select the file and click on versions tab you will see the versions
 - Also in S3 bucket level -> Objects section -> List Versions and you will see the versions of the file
 - you can select the version you want to rollback and delete that so it will be rolled back to the previous version
 - Deleting a file with list versions enabled will do a soft delete.
   - To get back this file you have to enable List versions and then you will see the deleted file with "Delete marker" and then select delete tp restore

## 7.4 S3 Access logs

**basics**  
 - For audit purpose, you may want to log all access to S3 buckets
 - Any request made to S3, from any account, authorized or denied will be logged into another S3 bucket
 - That data can be analyzed using data analysis tools..
 - Very helpful to come down to the root cause of an issue, or audit usage, view suspicious patterns, etc..

**Hands on**  
 - To enable logging you have to create a new bucket where access logs will be written
 - Go to original S3 bucket -> Properties -> Server access logging setting -> edit -> enable and provide the target bucket that we created above.
 - You can access the objects and then these logs will be reflected in another s3 bucket after an hour or two.

## 7.5 S3 Replication (CRR and SRR)

**basics**  
 - Must enable versioning in source and destination
 - Cross Region Replication (CRR) 
 - Same Region Replication (SRR)
 - Buckets can be in different accounts
 - Copying is asynchronous
 - Must give proper IAM permissions to S3
 - CRR use cases
   - Compliance
   - lower latency access
   - Replication across accounts
 - SRR use cases
   - log aggregation
   - live replication between production and test accounts

**hands on**  
 - create a new S3 bucket where you need data to be replicated
 - enable versioning in source and target bucket
 - go to main bucket -> management tab -> Replication rule -> Create replication rule
   - provide rule name
   - status : enabled
   - Source bucket
   - Destination: buckte in this account, provide the name
   - IAM role: Create a new role
   - Save
   - Replication rule created
 - Replication will work only for objects that are being uploaded after enabling the replication rule

## 7.6 S3 Storage classes

**Types**  
 - Amazon S3 Standard - General purpose
 - Amazon S3 Standard-Infrequent Access (not accessed ver often)
 - Amazon S3 one Zone-Infrequent Access (you can recreate and fine with losing it)
 - Amazon S3 Intelligent tiering (Not sure which one to choose between frquent and infrequent access)
 - Amazon Glacier (Backups and archives)
 - Amazon Glacier Deep Archive (Backups and archives)

**Durability and Availability**  
**Durability:**  
 - High durability (99.99999999999, 11 time 9) of objects across multiple Azure
 - If you store 10,000,000 objects with Amazon S3, you can on average expect to incur a loss of a single object once every 10,000 years
 - `Same for all storage classess`

**Availability**  
 - Measures how readily available a service is
 - S3 standard has 99.99% availability, which means it wil not be available 53 minutes a year
 - `Varies depending on storage class`

***1. Amazon S3 Standard - General purpose**  
 - 99.99% availability
 - Used for frequently accessed data
 - low latency and high throughput
 - sustain 2 concurrent facility failures
 - Use cases:
   - Big Data analytics
   - Mobile and gamin applications
   - content distribution

***2. Amazon S3 Standard-Infrequent Access (IA)**  
 - Suitable for data that is less frequently accessed, but requires rapid access when needed
 - 99.9% Availability
 - Lower cost compared to Amazon S3 standard, but retrieval fee
 - Sustain 2 concurrent facility failures
 - Use Cases: 
   - As a data store for disaster recovery
   - backups

***3. Amazon S3 one Zone-Infrequent Access (IA)**  
 - Same as IA but data is stored in a single Azure
 - 99.5% Availability
 - Low latency and high throughput performance
 - Lower cost compared to S3-IA (by 20%)
 - Use Cases:
   - Storing secondary backup copies of on premise data or
   - storing data you can recreate

***4. Amazon S3 Intelligent tiering**  
 - 99.9% Availability
 - Same low latency and high throughput performance of S3 Standard
 - Cost optimized by automatically moving objects between two access tiers based on changing access patterns:
   - frequent access
   - Infrequent access
 - Resilient against events that impact an entire Availability zone

***5. Amazon Glacier and Amazon Glacier Deep Archive**  
 - Low cost object storage (in GB/month) meant for archiving/backup
 - Data is retained for the longer term (years)
 - Various retrieval options of time + fees for retrieval
 - Amazon Glacier: Cheap
   - Expedited (1 to 5 minutes)
   - Standard (3 to 5 hours)
   - Bulk (5 to 12 hours)
 - Amazon Glacier Deep Archive: Cheapest
   - Standard (12 hours)
   - Bulk (48 hours)

**Summary**  
 - https://aws.amazon.com/s3/storage-classes/

**Moving between storage classes**  
 - You can transition objects between storage classes
 - For infrequently accessed object, move them to Standard_IA
 - For archive objects you dont need in real-time, GLACIER or DEEP_ARCHIVE
 - Moving Objects can be automated using a lifecycle configuration

**Hands on**  
 - S3 -> Objects -> upload -> Additional upload options, here you can select the storage class and details are also available on the same -> upload
 - if you select the uploaded file, yyou can verify the storage class and you can update the same if needed
 - S3 -> Buckets -> Select bucket -> management tab -> Create lifecycle rule
   - LifeCycle Rule name
   - Choose a rule scope
   - Lifecycle Rule actions: depending on the checkboxes you select, you will be provided with the options to configure below
   - Create Rule

## 7.7 Snowball, Snowball Edge and SnowMobile Overview

**Snowball**  
 - Physical data transport solution that heps moving TBs or PBs of data in or out of AWS
 - Alternative to moving data over the network (and paying network fees)
 - pay per data transfer jobs
 - Use cases:
   - LArge data cloud migrations
   - Data center decommision
   - Disaster recovery
 - if it takes more than a week to transfer over the network, use snowball devices

**Snowball Process**  
 - Request snowball devices from the AWS console for delivery
 - Install the snowball client on your servers
 - Connect the snowball to your servers and copy files using the client
 - Ship back the device when you are done (goes to the right AWS facility)
 - Data will be loaded into an S3 bucket
 - Snowball is completely wiped

**Snowball Edge**  
 - Snowball Edge (100 TB) add computational capability to the device
 - Supports a custom EC2 AMI so you cab perform processing on the go
 - Supports customm lambda functions
 - Very useful to pre-process the data while moving
 - Use case:
   - data migrations
   - image collation
   - IoT capture
   - Machine Learning
 - "Snowball" is deprecated in favour of "Snowball Edge"

**AWS Snowmobile (Truck)**  
 - Transfer exabytes of data (1 EB = 1,000 PB = 10,000,00 TBs)
 - Each Snowmobile has 100PB of capacity (use multiple in parallel)
 - Better than snowball if you transfer more than 10 PB

## 7.8 Storage gateway overview

**Where do we need?**  
 - AWS is pushing for "hybrid cloud"
   - Part of your infrastructure is on-premises
   - Part of your infrastructure is on the cloud
 - This can be due to
   - Long cloud migrations
   - Security requirements
   - Compliance requirements
   - IT strategy
 - S3 is a propritary storage technology (unlike EFS/NFS), **so how do you expose the S3 data in-premise?**
    - via AWS Storage Gateway

**AWS Storage Cloud Native Options**  
 - Block storage
   - Amazon EBS
   - EC2 instance store
 - File Syorage
   - Amazon EFS
 - Object storage
   - Amazon S3
   - Glacier

**AWS storage gateway**  
 - Bridge between on-premise data and cloud data in S3
 - Hybrid storage service to allow on-premises to seamlessly use the AWS Cloud
 - Use cases:
   - Disaster recovery
   - Backup and restore
   - Tiered storage
 - Types of storage gateway
   - File gateway
   - Volume gateway
   - Tape gateway
 - No need to know the types at the exam

## 7.9 Shared responsibility model for S3

| AWS                                                 | You                           |
| -------------                                       |:-------------:                      |
| Infrastructure (global security, durability, availability, sustain concurrent loss of data in two facilities) | S3 Versioning |
| Configuration and vulnerability analysis            | S3 Bucket Policies|
| Compliance Validation                               | S3 Replication Setup |
|                                                     | Logging and Monitoring |
|                                                     | S3 Storage Classes |
|                                                     | Data encryption at rest and in transit |

## 7.10 S3 Summary

 - Buckets vs Objects: global unique name, tied to a region
 - S3 security: IAM policy, S3 bucket Policy (public access), S3 encryption
 - S3 Websites: host a static website on Amazon S3
 - S3 Versioning: Multiple versions for files, prevents accidental deletes
 - S3 Access Logs: Log requests made within your S3 buckets
 - S3 Replication: Same-region or Cross-Region, must enable versioning
 - S3 Storage Classes: Standard. IA, IZ-IA, Intelligent, Glacier, Deep Archive
 - S3 Lifecycle Rules: transition objects between classes
 - Snowball/Snowmobile: import data into S3 through a physical device
 - Storage gateway: hybrid solution to extend on-premises storage to S3

## S3 Encryption
Their are 4 methods of encrypting objects in s3:
 - SSE-S3: Encrypts S3 objects using keys handled and managed by aws
 - SSE-KMS: leverage aws key management service to manage encryption keys
 - SSE-C: When you want to manage your encryption keys
 - Client Side encryption

### SSE-S3
 - Encryption using keys handled and managed by aws
 - Object is encrypted server side
 - AES-256 encryption type
 - Must set header: "x-amz-server-side-encryption":"AES256"

### SSE-KMS
 - Encryption using keys managed and handles by KMS
 - Object is encrypted server side
 - KMS advantages: User control + audit trail
 - Must set header: "x-amz-server-side-encryption":"aws:kms"

### SSE-C
 - Encryption using data keys fully managed by the customer outside of AWS
 - Amazon S3 does not store the encryption keys you provide
 - HTTPS must be used
 - Encryption key must be provided in headers, for every HTTP request made

### Client Side encryption
 - Cleint library such as Amazon S3 encryption client
 - Clients must encrypt data themselves before sending to S3
 - Clients must decrypt data themselves when retrieving from S3
 - Customer fully manages the keys and encryption cycle

### Encryption in Transit:
 - Amazon S3 exposes:
   - HTTP endpoint: non encrypted
   - HTTPS endpoint: encryption in flight
 - You are free to use the endpoint you want, but HTTPS is recommended
 - Most clients would use the HTTPS endpoint by default
 - HTTPS is mandatory for SSE-C
 - Encryption in flight is also called SSL/TLS




 


